{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MVKh-rqFih2V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor([\n",
        "    [0.72, 0.45, 0.31],\n",
        "    [0.75, 0.20, 0.55],\n",
        "    [0.30, 0.80, 0.40],\n",
        "    [0.85, 0.35, 0.60],\n",
        "    [0.55, 0.15, 0.75],\n",
        "    [0.25, 0.20, 0.85]\n",
        "])"
      ],
      "metadata": {
        "id": "ddmZnaBzjcMV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkDeY5h8jdxG",
        "outputId": "27abd6c7-c9a7-401e-f6df-81043ae405a8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, _ = x.shape\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(1, 2)\n",
        "        attn_scores.masked_fill_(\n",
        "            self.mask.bool()[:num_tokens, :num_tokens],\n",
        "            -torch.inf\n",
        "        )\n",
        "\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5,\n",
        "            dim=-1\n",
        "        )\n",
        "\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec\n"
      ],
      "metadata": {
        "id": "UKiGrIiajhim"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [\n",
        "                CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
        "                for _ in range(num_heads)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([head(x) for head in self.heads], dim=-1)\n"
      ],
      "metadata": {
        "id": "4qOfCcFRjjWU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "context_length = batch.shape[1]\n",
        "d_in, d_out = 3, 2\n",
        "\n",
        "mha = MultiHeadAttentionWrapper(\n",
        "    d_in=d_in,\n",
        "    d_out=d_out,\n",
        "    context_length=context_length,\n",
        "    dropout=0.0,\n",
        "    num_heads=2\n",
        ")"
      ],
      "metadata": {
        "id": "8aQ861lYjkti"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "id": "JTTglVXCjml5",
        "outputId": "3da94c4b-2fbd-432f-87e0-614f9eba20b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.5762, -0.1627,  0.5569,  0.3635],\n",
            "         [-0.5650, -0.0630,  0.5599,  0.3006],\n",
            "         [-0.5472, -0.1226,  0.5285,  0.3435],\n",
            "         [-0.5787, -0.0943,  0.5621,  0.3388],\n",
            "         [-0.5593, -0.0436,  0.5509,  0.3046],\n",
            "         [-0.5287, -0.0033,  0.5277,  0.2743]],\n",
            "\n",
            "        [[-0.5762, -0.1627,  0.5569,  0.3635],\n",
            "         [-0.5650, -0.0630,  0.5599,  0.3006],\n",
            "         [-0.5472, -0.1226,  0.5285,  0.3435],\n",
            "         [-0.5787, -0.0943,  0.5621,  0.3388],\n",
            "         [-0.5593, -0.0436,  0.5509,  0.3046],\n",
            "         [-0.5287, -0.0033,  0.5277,  0.2743]]], grad_fn=<CatBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QpJacR-djoIB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}