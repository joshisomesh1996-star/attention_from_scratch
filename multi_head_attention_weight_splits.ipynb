{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MVKh-rqFih2V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor([\n",
        "    [0.72, 0.45, 0.31],\n",
        "    [0.75, 0.20, 0.55],\n",
        "    [0.30, 0.80, 0.40],\n",
        "    [0.85, 0.35, 0.60],\n",
        "    [0.55, 0.15, 0.75],\n",
        "    [0.25, 0.20, 0.85]\n",
        "])"
      ],
      "metadata": {
        "id": "ddmZnaBzjcMV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkDeY5h8jdxG",
        "outputId": "9192174a-f938-4ddd-deae-d6970e00c7df"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, _ = x.shape\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(1, 2)\n",
        "        attn_scores.masked_fill_(\n",
        "            self.mask.bool()[:num_tokens, :num_tokens],\n",
        "            -torch.inf\n",
        "        )\n",
        "\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5,\n",
        "            dim=-1\n",
        "        )\n",
        "\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec\n"
      ],
      "metadata": {
        "id": "UKiGrIiajhim"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [\n",
        "                CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
        "                for _ in range(num_heads)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([head(x) for head in self.heads], dim=-1)\n"
      ],
      "metadata": {
        "id": "4qOfCcFRjjWU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "context_length = batch.shape[1]\n",
        "d_in, d_out = 3, 2\n",
        "\n",
        "mha = MultiHeadAttentionWrapper(\n",
        "    d_in=d_in,\n",
        "    d_out=d_out,\n",
        "    context_length=context_length,\n",
        "    dropout=0.0,\n",
        "    num_heads=2\n",
        ")"
      ],
      "metadata": {
        "id": "8aQ861lYjkti"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTTglVXCjml5",
        "outputId": "bddf0df6-8ce0-4465-a43e-8ffaab00e27c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.5762, -0.1627,  0.5569,  0.3635],\n",
            "         [-0.5650, -0.0630,  0.5599,  0.3006],\n",
            "         [-0.5472, -0.1226,  0.5285,  0.3435],\n",
            "         [-0.5787, -0.0943,  0.5621,  0.3388],\n",
            "         [-0.5593, -0.0436,  0.5509,  0.3046],\n",
            "         [-0.5287, -0.0033,  0.5277,  0.2743]],\n",
            "\n",
            "        [[-0.5762, -0.1627,  0.5569,  0.3635],\n",
            "         [-0.5650, -0.0630,  0.5599,  0.3006],\n",
            "         [-0.5472, -0.1226,  0.5285,  0.3435],\n",
            "         [-0.5787, -0.0943,  0.5621,  0.3388],\n",
            "         [-0.5593, -0.0436,  0.5509,  0.3046],\n",
            "         [-0.5287, -0.0033,  0.5277,  0.2743]]], grad_fn=<CatBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: (batch=1, seq_len=3, d_model=6)\n",
        "x = torch.tensor([[\n",
        "    [1.0, 2.0, 3.0, 4.0, 5.0, 6.0],\n",
        "    [6.0, 5.0, 4.0, 3.0, 2.0, 1.0],\n",
        "    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
        "]])\n",
        "\n",
        "batch_size, seq_len, d_model = x.shape\n"
      ],
      "metadata": {
        "id": "QpJacR-djoIB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "Wq = torch.randn(d_model, d_model)\n",
        "Wk = torch.randn(d_model, d_model)\n",
        "Wv = torch.randn(d_model, d_model)\n"
      ],
      "metadata": {
        "id": "gjxP1zwWIGLI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q = x @ Wq\n",
        "K = x @ Wk\n",
        "V = x @ Wv"
      ],
      "metadata": {
        "id": "NWtxXYAkILW5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_heads = 2\n",
        "head_dim = d_model // num_heads"
      ],
      "metadata": {
        "id": "39W1QSvcINul"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q = Q.view(batch_size, seq_len, num_heads, head_dim)\n",
        "K = K.view(batch_size, seq_len, num_heads, head_dim)\n",
        "V = V.view(batch_size, seq_len, num_heads, head_dim)"
      ],
      "metadata": {
        "id": "8hAVXfDLIP9n"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q = Q.transpose(1, 2)\n",
        "K = K.transpose(1, 2)\n",
        "V = V.transpose(1, 2)"
      ],
      "metadata": {
        "id": "WFyiu2PWITpO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K_T = K.transpose(2, 3)\n",
        "attn_scores = Q @ K_T"
      ],
      "metadata": {
        "id": "W0Kdsr81IXod"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
        "attn_scores.masked_fill_(mask, -torch.inf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfJwcoYeIbNQ",
        "outputId": "743a026d-730b-4aea-de93-53a45ae518c9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-318.2692,      -inf,      -inf],\n",
              "          [-294.6535,    9.5538,      -inf],\n",
              "          [ -87.5604,   -1.7744,  -12.7621]],\n",
              "\n",
              "         [[ 116.1476,      -inf,      -inf],\n",
              "          [ 178.4425,  171.2106,      -inf],\n",
              "          [  42.0843,   31.7945,   10.5541]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(attn_scores / head_dim**0.5, dim=-1)"
      ],
      "metadata": {
        "id": "2lr0FzTFIcyK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dropout = nn.Dropout(0.1)\n",
        "attn_weights = dropout(attn_weights)"
      ],
      "metadata": {
        "id": "gWkvPR0sIgZe"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_vec = attn_weights @ V"
      ],
      "metadata": {
        "id": "ygY7AbSlIiC5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_vec = context_vec.transpose(1, 2)"
      ],
      "metadata": {
        "id": "zqMZu0-pIjm2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_vec = context_vec.reshape(batch_size, seq_len, num_heads * head_dim)"
      ],
      "metadata": {
        "id": "h_tf4wljImdO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, _ = x.shape\n",
        "\n",
        "        Q = self.W_query(x)\n",
        "        K = self.W_key(x)\n",
        "        V = self.W_value(x)\n",
        "\n",
        "        Q = Q.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        attn_scores = Q @ K.transpose(2, 3)\n",
        "        attn_scores.masked_fill_(\n",
        "            self.mask[:num_tokens, :num_tokens].bool(),\n",
        "            -torch.inf\n",
        "        )\n",
        "\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / self.head_dim**0.5,\n",
        "            dim=-1\n",
        "        )\n",
        "\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "        context = attn_weights @ V\n",
        "\n",
        "        context = context.transpose(1, 2).reshape(b, num_tokens, self.d_out)\n",
        "        output = self.out_proj(context)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "q6YuUTOIIn_h"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "mha = MultiHeadAttention(\n",
        "    d_in=6,\n",
        "    d_out=6,\n",
        "    context_length=seq_len,\n",
        "    dropout=0.1,\n",
        "    num_heads=2\n",
        ")\n",
        "\n",
        "out = mha(x)\n",
        "print(out)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrPobF8SIqc-",
        "outputId": "2b2add80-1436-425f-e6cf-f0d640de061f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1556,  0.0844,  1.1247,  0.6199, -1.1666, -0.0814],\n",
            "         [ 0.1454,  0.0948,  1.1142,  0.6164, -1.1659, -0.0827],\n",
            "         [-0.1063,  0.3392,  0.3488,  0.2884, -0.7438, -0.1504]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "torch.Size([1, 3, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”· Multi-Head Attention (Mapped Directly to Code)\n",
        "\n",
        "**Initialize dimensions** by enforcing `d_out % num_heads == 0` so that *each attention head has equal capacity*.  \n",
        "Compute the per-head dimension as **`head_dim = d_out // num_heads`**.\n",
        "\n",
        "---\n",
        "\n",
        "**Define learnable linear projections**  \n",
        "The layers **`W_query`**, **`W_key`**, and **`W_value`** project the input embeddings into:\n",
        "- **Query (Q)**\n",
        "- **Key (K)**\n",
        "- **Value (V)**  \n",
        "\n",
        "Each projection produces tensors of shape **`(b, num_tokens, d_out)`**.\n",
        "\n",
        "---\n",
        "\n",
        "**Register a causal mask**  \n",
        "A **causal upper-triangular mask** is registered using `register_buffer`.  \n",
        "This mask:\n",
        "- Is **stored with the model**\n",
        "- Is **not trainable**\n",
        "- Prevents tokens from attending to **future positions**\n",
        "\n",
        "---\n",
        "\n",
        "**Forward pass begins**  \n",
        "The input tensor **`x`** has shape **`(b, num_tokens, d_in)`**.  \n",
        "Compute the projections:\n",
        "- `Q = W_query(x)`\n",
        "- `K = W_key(x)`\n",
        "- `V = W_value(x)`\n",
        "\n",
        "---\n",
        "\n",
        "**Split into multiple heads (reshape)**  \n",
        "Each projection is reshaped to:\n",
        "\n",
        "**`(b, num_tokens, num_heads, head_dim)`**\n",
        "\n",
        "This step:\n",
        "- Splits the embedding dimension\n",
        "- Assigns a smaller subspace to each head\n",
        "- *Does not change the underlying values*\n",
        "\n",
        "---\n",
        "\n",
        "**Group by head (transpose)**  \n",
        "Transpose each tensor to:\n",
        "\n",
        "**`(b, num_heads, num_tokens, head_dim)`**\n",
        "\n",
        "This reordering allows **attention to be computed independently for each head**.\n",
        "\n",
        "---\n",
        "\n",
        "**Scaled dot-product attention**  \n",
        "Compute attention scores using:\n",
        "\n",
        "`Q @ K.transpose(2, 3)`\n",
        "\n",
        "This produces scores of shape **`(b, num_heads, num_tokens, num_tokens)`**.  \n",
        "Apply the **causal mask** to block future tokens, then apply **softmax scaled by `sqrt(head_dim)`** to obtain normalized attention weights.\n",
        "\n",
        "---\n",
        "\n",
        "**Dropout and context computation**  \n",
        "Apply **dropout** to the attention weights for regularization.  \n",
        "Compute the context vectors using:\n",
        "\n",
        "`attn_weights @ V`\n",
        "\n",
        "Resulting shape: **`(b, num_heads, num_tokens, head_dim)`**\n",
        "\n",
        "---\n",
        "\n",
        "**Merge heads back together**  \n",
        "Transpose the context back to:\n",
        "\n",
        "**`(b, num_tokens, num_heads, head_dim)`**\n",
        "\n",
        "Then reshape to:\n",
        "\n",
        "**`(b, num_tokens, d_out)`**\n",
        "\n",
        "This concatenates all heads into a single representation.\n",
        "\n",
        "---\n",
        "\n",
        "**Final output projection**  \n",
        "Apply the output projection **`out_proj`** to mix information from all heads and produce the final output:\n",
        "\n",
        "**`(b, num_tokens, d_out)`**\n"
      ],
      "metadata": {
        "id": "bpyInQKnPyIo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QS0kyVhMIr_8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}